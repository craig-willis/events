package edu.gslis.events.main;

import java.io.File;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.TreeMap;

import org.apache.commons.io.FileUtils;

import com.google.common.collect.Ordering;

import edu.gslis.docscoring.support.CollectionStats;
import edu.gslis.docscoring.support.IndexBackedCollectionStats;
import edu.gslis.indexes.IndexWrapper;
import edu.gslis.indexes.IndexWrapperFactory;
import edu.gslis.lucene.indexer.Indexer;
import edu.gslis.queries.GQuery;
import edu.gslis.queries.expansion.FeedbackRelevanceModel;
import edu.gslis.searchhits.SearchHit;
import edu.gslis.searchhits.SearchHits;
import edu.gslis.temporal.scorers.KDEScorer;
import edu.gslis.temporal.util.ValueComparableMap;
import edu.gslis.textrepresentation.FeatureVector;
import edu.gslis.utils.Stopper;


public class FindEventsTempMI 
{
    static final double ALPHA = 0.5;
    static final double MU = 2500;
    static final int NUM_TERMS = 10;
    static final int NUM_DOCS = 50;
    
    IndexWrapper index = null;
    IndexWrapper wpIndex = null;
    Map<String, Double> acfMap = null;
    Map<String, Map<String, Double>> miMap = null;
    static int colStart = 0;
    static int colEnd = 0;
    static int colInterval = 0;

    Stopper stopper;
    CollectionStats colStats;
    CollectionStats wpColStats;
    
    public static void main(String[] args) throws Exception {
        
        String acfPath = args[0];
        String tmiPath = args[1];
        
        String indexPath = args[2];
        String stopperPath = args[3];
        String wpIndexPath = args[4];
        
        colStart = Integer.parseInt(args[5]);
        colEnd = Integer.parseInt(args[6]);
        colInterval = Integer.parseInt(args[7]); 

        FindEventsTempMI finder = new FindEventsTempMI(acfPath, tmiPath, indexPath, stopperPath, wpIndexPath);
        finder.findEvents();
    }
    
    public FindEventsTempMI(String acfPath, String tmiPath, String indexPath, String stopperPath, 
            String wpIndexPath) 
            throws Exception 
    {
        stopper= new Stopper(stopperPath);
        index = IndexWrapperFactory.getIndexWrapper(indexPath);
        index.setTimeFieldName(Indexer.FIELD_EPOCH);
        acfMap = readTermAcf(acfPath);
        miMap = readTempMi(tmiPath);
        
        wpIndex = IndexWrapperFactory.getIndexWrapper(wpIndexPath);
        wpIndex.setTimeFieldName(null);
        wpColStats = new IndexBackedCollectionStats();
        wpColStats.setStatSource(wpIndexPath);

        colStats = new IndexBackedCollectionStats();
        colStats.setStatSource(indexPath);

    }
    
    public void findEvents() throws Exception 
    {
        // Build a set of language models based on temporal mutual information
        Map<String, FeatureVector> lms = new TreeMap<String, FeatureVector>();
        for (String f1: acfMap.keySet()) 
        {
            FeatureVector lm = new FeatureVector(stopper);
            
            Map<String, Double> miTerms = miMap.get(f1);
            lm.addTerm(f1, 0.1);
            if (miTerms == null) {
                System.err.println("Missing mutual info data for " + f1);
                continue;
            }
            

            for (String f2: miTerms.keySet())
                lm.addTerm(f2, miTerms.get(f2));
            
            lm.clip(20);
            lm.normalize();
            System.out.println("\n" + f1 + "\n" + lm.toString());
            lms.put(f1, lm);
        }

        
        // Question: are two features generated by the same underlying event?
        // Do they return the same Wikipedia page?
        

        for (String f1: acfMap.keySet()) 
        {         
            if (lms.get(f1) != null) 
            {
                System.out.println("\n" + f1 + "\n" + lms.get(f1).toString(10));
                GQuery wpQuery = new GQuery();
                wpQuery.setFeatureVector(lms.get(f1));
                wpQuery.setTitle(f1);
                SearchHits hits = wpIndex.runQuery(wpQuery, 10);
                for (SearchHit hit: hits.hits()) {                
                    System.out.println(hit.getDocno());
                }
            }
        }
    }    
    
    
    double tmi(String f1, String f2) {
        double mi = 0;       
        Map<String, Double> miTerms = miMap.get(f1);
        try {
            if (miTerms.containsKey(f2)) {
                mi = miTerms.get(f2);
            }
        } catch (Exception e) {
            
        }
        return mi;
    }



    public double mdis(FeatureVector fv1, FeatureVector fv2, Set<String> features) {
        double mdis = 0;
        
        double N = features.size();
        
        for (String feature: features) {
            double p1 = fv1.getFeatureWeight(feature);
            double p2 = fv2.getFeatureWeight(feature);
            if (p1 > 0 && p2 > 0) 
                mdis += N * p1 * Math.log(p1/p2)/Math.log(2);
        }        
        return 2*mdis;
    }
    
    
    public double kl(FeatureVector fv1, FeatureVector fv2) {
        double ll = 0;
        
        Set<String> features = new HashSet<String>();
        features.addAll(fv1.getFeatures());
        features.addAll(fv2.getFeatures());
        
        for (String feature: features) {
            double w2 = fv2.getFeatureWeight(feature);
            double l2 = fv2.getLength();
            
            double cp = (index.termFreq(feature)) / index.termCount();
            double pr = (w2 + NUM_TERMS*cp)/(l2 + NUM_TERMS);

            double w1 = fv1.getFeatureWeight(feature);
            ll += w1 * Math.log(pr);                        
        }        
        return ll;
    }
   
    public String getQueryTitle(FeatureVector fv) {
        String title = "";
        for (String f: fv.getFeatures()) {
            title += f + " ";
        }
        return title.trim();
    }
    public static FeatureVector buildRm(FeatureVector qv, Stopper stopper, IndexWrapper index, CollectionStats collStats) 
    {
        //FeatureVector qv = new FeatureVector(term, stopper);
        GQuery gquery = new GQuery();
        gquery.setFeatureVector(qv);
//        gquery.setText(term);
        gquery.setTitle("");
        
        SearchHits hits = index.runQuery(gquery, NUM_DOCS);
        KDEScorer kde = new KDEScorer();
        kde.setStartTime(colStart);
        kde.setEndTime(colEnd);
        kde.setQuery(gquery);
        kde.setParameter("alpha", ALPHA);
        kde.setParameter("mu", MU);
        kde.init(hits);
        kde.setCollectionStats(collStats);
        Iterator<SearchHit> it = hits.iterator();
        while (it.hasNext()) {
            SearchHit hit = it.next();
            double score = kde.score(hit);
            hit.setScore(score);
        }
        hits.rank();
        
        FeedbackRelevanceModel rm = new FeedbackRelevanceModel();
        rm.setTermCount(NUM_TERMS);
        rm.setDocCount(NUM_DOCS);
        rm.setRes(hits);
        rm.setIndex(index);
        rm.setOriginalQuery(gquery);
        rm.setStopper(stopper);
        rm.build();
        //FeatureVector rm3 = FeatureVector.interpolate(qv, rm.asFeatureVector(), 0.5);
        FeatureVector rm3 = rm.asFeatureVector();
        rm3.clip(NUM_TERMS);
        rm3.normalize();
        return rm3;
    }
    
    public static Map<String, Map<String, Double>> readTempMi(String miTermPath) throws Exception 
    {
        Map<String, Map<String, Double>> miTerms = new HashMap<String, Map<String, Double>>();
        List<String> lines = FileUtils.readLines(new File(miTermPath));
        for (String line: lines) {
            String[] fields = line.split(",");
            if (fields.length == 3) {
                String term1 = fields[0];
                String term2 = fields[1];
                double mi = Double.parseDouble(fields[2]);
                Map<String, Double> map = miTerms.get(term1);
                if (map == null)
                    map = new ValueComparableMap<String, Double>(Ordering.natural().reverse());
                map.put(term2, mi);
                miTerms.put(term1, map);
            }
        }
        
        // Normalize
        for (String term1: miTerms.keySet()) {
            Map<String, Double> values = miTerms.get(term1);
            double sum = 0;
            for (String term2: values.keySet())
                sum += values.get(term2);
            
            Map<String, Double> norm = new HashMap<String, Double>();
            for (String term2: values.keySet()) {
                norm.put(term2, values.get(term2)/sum);
            }
            miTerms.put(term1, norm);
        }
        return miTerms;
    }
    
    public static Map<String, Double> readTermAcf(String acfTermPath) throws Exception 
    {
        Map<String, Double> acfTerms = new ValueComparableMap<String, Double>(Ordering.natural().reverse());
        List<String> lines = FileUtils.readLines(new File(acfTermPath));
        for (String line: lines) {
            String[] fields = line.split(",");
            String term = fields[0];
            double acf = Double.parseDouble(fields[1]);
            acfTerms.put(term, acf);
        }
        return acfTerms;
    }
   
}
